{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c6fd9d27",
   "metadata": {},
   "source": [
    "# Dataset Audit and Export\n",
    "\n",
    "This notebook performs a structured audit of multiple computer vision datasets and prepares them for unified downstream use."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "98d339d0",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "\n",
    "The goal of this notebook is to validate dataset structure, identify inconsistencies, and export cleaned datasets in a standardized format suitable for training and evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "2ec62190",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Checking dataset paths under: /Users/Marco/Desktop/SmartFISHER/SmartFISHER_DATASET/Original_Datasets\n",
      "luderick           → OK  (/Users/Marco/Desktop/SmartFISHER/SmartFISHER_DATASET/Original_Datasets/InstanceSegmented_Luderick_dataset_yolov11)\n",
      "fish_416           → OK  (/Users/Marco/Desktop/SmartFISHER/SmartFISHER_DATASET/Original_Datasets/fish_dataset_416x416)\n",
      "aquarium           → OK  (/Users/Marco/Desktop/SmartFISHER/SmartFISHER_DATASET/Original_Datasets/Aquarium_Combined_roboflow_v6i.yolov8)\n",
      "f4k                → OK  (/Users/Marco/Desktop/SmartFISHER/SmartFISHER_DATASET/Original_Datasets/f4k_detection_tracking)\n",
      "fishclef           → OK  (/Users/Marco/Desktop/SmartFISHER/SmartFISHER_DATASET/Original_Datasets/fishclef_2015_release)\n",
      "deepfish           → OK  (/Users/Marco/Desktop/SmartFISHER/SmartFISHER_DATASET/Original_Datasets/Deepfish_Annotation)\n",
      "AquaCoop           → OK  (/Users/Marco/Desktop/SmartFISHER/SmartFISHER_DATASET/Original_Datasets/Vasconcelos_Thesis_DS/Seabass_Aquaculture_640)\n",
      "OzFish             → OK  (/Users/Marco/Desktop/SmartFISHER/SmartFISHER_DATASET/Original_Datasets/Vasconcelos_Thesis_DS/OzFish_cleaned)\n",
      "deepfish_negatives → OK  (/Users/Marco/Desktop/SmartFISHER/SmartFISHER_DATASET/Original_Datasets/Deepfish_Annotation/Negative_samples)\n",
      "\n",
      "[INFO] All dataset paths verified successfully.\n"
     ]
    }
   ],
   "source": [
    "import os, glob, hashlib, math, shutil\n",
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "\n",
    "# ----------------- Configuration -----------------\n",
    "ROOT = Path(\"/Users/Marco/Desktop/SmartFISHER/SmartFISHER_DATASET/Original_Datasets\")  # Root folder containing all datasets\n",
    "\n",
    "# Map dataset keys to subfolders under ROOT\n",
    "SOURCE_DATASETS = {\n",
    "    'luderick': 'InstanceSegmented_Luderick_dataset_yolov11',\n",
    "    'fish_416': 'fish_dataset_416x416',\n",
    "    'aquarium': 'Aquarium_Combined_roboflow_v6i.yolov8',\n",
    "    'f4k': 'f4k_detection_tracking',\n",
    "    'fishclef': 'fishclef_2015_release',\n",
    "    'deepfish': 'Deepfish_Annotation',\n",
    "    'AquaCoop': 'Vasconcelos_Thesis_DS/Seabass_Aquaculture_640',\n",
    "    'OzFish': 'Vasconcelos_Thesis_DS/OzFish_cleaned',\n",
    "    'deepfish_negatives': 'Deepfish_Annotation/Negative_samples',\n",
    "}\n",
    "\n",
    "# Datasets with sparse annotations (do not treat unlabeled frames as negatives)\n",
    "SPARSE_SOURCES = {'f4k', 'fishclef'}\n",
    "\n",
    "# Image extensions to scan (case-insensitive)\n",
    "IMG_EXTS = {'.jpg', '.jpeg', '.png', '.bmp', '.tif', '.tiff'}\n",
    "\n",
    "# ----------------- Outputs -----------------\n",
    "EXPORT_DIR          = Path('/Users/Marco/Desktop/SmartFISHER/SmartFISHER_DATASET/GitHub_Test')\n",
    "PICTURES_KEEP_CSV   = EXPORT_DIR / 'pictures_keep.csv'\n",
    "\n",
    "PICTURES_REMOVE_CSV = EXPORT_DIR / 'pictures_remove.csv'\n",
    "REVIEWED_NEG_CSV    = EXPORT_DIR / 'negatives_kept.csv'\n",
    "\n",
    "# Preferred source order for choosing representatives when duplicates exist\n",
    "PREFERRED_SOURCE_ORDER = [\n",
    "    \"deepfish_negatives\", \"deepfish\", \"fishclef\", \"f4k\",\n",
    "    \"luderick\", \"fish_416\", \"aquarium\", \"AquaCoop_OzFish\"\n",
    "]\n",
    "\n",
    "# Random seed for reproducibility (affects any sampling/random operations)\n",
    "RANDOM_SEED = 42\n",
    "np.random.seed(RANDOM_SEED)\n",
    "\n",
    "# Also set Python's built-in random module for any operations that might use it\n",
    "import random\n",
    "random.seed(RANDOM_SEED)\n",
    "\n",
    "# ----------------- Path Validation -----------------\n",
    "def check_paths(root: Path, dataset_map: dict):\n",
    "    \"\"\"Validate ROOT and dataset folder existence.\"\"\"\n",
    "    print(f\"[INFO] Checking dataset paths under: {root}\")\n",
    "    if not root.exists():\n",
    "        raise FileNotFoundError(f\"[ERROR] ROOT directory does not exist: {root}\")\n",
    "\n",
    "    missing = []\n",
    "    for name, rel_path in dataset_map.items():\n",
    "        full_path = root / rel_path\n",
    "        if full_path.exists():\n",
    "            print(f\"{name:18s} → OK  ({full_path})\")\n",
    "        else:\n",
    "            print(f\"{name:18s} → MISSING  ({full_path})\")\n",
    "            missing.append(name)\n",
    "\n",
    "    if missing:\n",
    "        print(f\"\\n[WARN] Missing dataset folders: {', '.join(missing)}\")\n",
    "    else:\n",
    "        print(\"\\n[INFO] All dataset paths verified successfully.\")\n",
    "\n",
    "# ----------------- Main Entry -----------------\n",
    "if __name__ == \"__main__\":\n",
    "    check_paths(ROOT, SOURCE_DATASETS)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e1b8755",
   "metadata": {},
   "source": [
    "## 1) Define Dataset Size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "71661249",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>path</th>\n",
       "      <th>exists</th>\n",
       "      <th>n_images</th>\n",
       "      <th>n_labels</th>\n",
       "      <th>n_bboxes</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f4k</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>True</td>\n",
       "      <td>77235</td>\n",
       "      <td>917</td>\n",
       "      <td>3460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fishclef</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>True</td>\n",
       "      <td>53196</td>\n",
       "      <td>14809</td>\n",
       "      <td>23294</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>deepfish</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>True</td>\n",
       "      <td>6517</td>\n",
       "      <td>6518</td>\n",
       "      <td>15464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>luderick</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>True</td>\n",
       "      <td>4276</td>\n",
       "      <td>8554</td>\n",
       "      <td>18881</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deepfish_negatives</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>True</td>\n",
       "      <td>2012</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fish_416</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>True</td>\n",
       "      <td>1350</td>\n",
       "      <td>1352</td>\n",
       "      <td>3183</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AquaCoop</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>True</td>\n",
       "      <td>1250</td>\n",
       "      <td>1250</td>\n",
       "      <td>13840</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>aquarium</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>True</td>\n",
       "      <td>638</td>\n",
       "      <td>640</td>\n",
       "      <td>4854</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>OzFish</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>True</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>7540</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              dataset                                               path  \\\n",
       "0                 f4k  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...   \n",
       "1            fishclef  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...   \n",
       "2            deepfish  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...   \n",
       "3            luderick  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...   \n",
       "4  deepfish_negatives  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...   \n",
       "5            fish_416  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...   \n",
       "6            AquaCoop  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...   \n",
       "7            aquarium  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...   \n",
       "8              OzFish  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...   \n",
       "\n",
       "   exists  n_images  n_labels  n_bboxes  \n",
       "0    True     77235       917      3460  \n",
       "1    True     53196     14809     23294  \n",
       "2    True      6517      6518     15464  \n",
       "3    True      4276      8554     18881  \n",
       "4    True      2012      2012         0  \n",
       "5    True      1350      1352      3183  \n",
       "6    True      1250      1250     13840  \n",
       "7    True       638       640      4854  \n",
       "8    True       350       350      7540  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrote dataset sizes to: /Users/Marco/Desktop/SmartFISHER/SmartFISHER_DATASET/GitHub_Test/dataset_sizes.csv\n"
     ]
    }
   ],
   "source": [
    "LABEL_EXTS = {'.txt', '.xml', '.json'}  # extend if you use other label formats\n",
    "\n",
    "# Skip any dataset path that looks like the unified export\n",
    "SKIP_KEYWORD = 'UNIFIED_DATASET_FISH'\n",
    "\n",
    "rows = []\n",
    "for name, rel in SOURCE_DATASETS.items():\n",
    "    ds_path = Path(ROOT) / rel\n",
    "\n",
    "    # Explicitly skip any dataset that points to the unified dataset export\n",
    "    if SKIP_KEYWORD in str(ds_path):\n",
    "        print(f\"[INFO] Skipping dataset '{name}' because path contains '{SKIP_KEYWORD}': {ds_path}\")\n",
    "        rows.append({\n",
    "            'dataset': name,\n",
    "            'path': str(ds_path),\n",
    "            'exists': False,\n",
    "            'n_images': 0,\n",
    "            'n_labels': 0,\n",
    "            'n_bboxes': 0,\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    if not ds_path.exists():\n",
    "        rows.append({\n",
    "            'dataset': name,\n",
    "            'path': str(ds_path),\n",
    "            'exists': False,\n",
    "            'n_images': 0,\n",
    "            'n_labels': 0,\n",
    "            'n_bboxes': 0,\n",
    "        })\n",
    "        continue\n",
    "\n",
    "    n_images = 0\n",
    "    n_labels = 0\n",
    "    # Count of bounding boxes (total non-empty lines in .txt YOLO label files)\n",
    "    n_bboxes = 0\n",
    "\n",
    "    for p in ds_path.rglob('*'):\n",
    "        if not p.is_file():\n",
    "            continue\n",
    "        suf = p.suffix.lower()\n",
    "        if suf in IMG_EXTS:\n",
    "            n_images += 1\n",
    "        elif suf in LABEL_EXTS:\n",
    "            n_labels += 1\n",
    "            # If it's a plain YOLO .txt label file, count non-empty lines as bboxes\n",
    "            if suf == '.txt':\n",
    "                try:\n",
    "                    with open(p, 'r', encoding='utf-8') as lf:\n",
    "                        for ll in lf:\n",
    "                            if ll.strip():\n",
    "                                n_bboxes += 1\n",
    "                except Exception:\n",
    "                    # If we can't read the label file, skip counting but keep the label file count\n",
    "                    pass\n",
    "\n",
    "    rows.append({\n",
    "        'dataset': name,\n",
    "        'path': str(ds_path),\n",
    "        'exists': True,\n",
    "        'n_images': n_images,\n",
    "        'n_labels': n_labels,\n",
    "        'n_bboxes': n_bboxes,\n",
    "    })\n",
    "\n",
    "# Build DataFrame with only the columns required by the pipeline\n",
    "df_sizes = pd.DataFrame(rows)\n",
    "# Include n_bboxes in display columns\n",
    "display_cols = ['dataset', 'path', 'exists', 'n_images', 'n_labels', 'n_bboxes']\n",
    "if not df_sizes.empty:\n",
    "    display(df_sizes[display_cols].sort_values('n_images', ascending=False).reset_index(drop=True))\n",
    "\n",
    "# Save to EXPORT_DIR if present, else save locally\n",
    "try:\n",
    "    out_dir = Path(EXPORT_DIR)\n",
    "except NameError:\n",
    "    out_dir = None\n",
    "\n",
    "out_csv = Path('dataset_sizes.csv') if out_dir is None else (out_dir / 'dataset_sizes.csv')\n",
    "if out_dir is not None:\n",
    "    out_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "# Write the CSV with the reduced columns so it integrates with the pipeline\n",
    "# Ensure n_bboxes is included in the written CSV\n",
    "cols_to_write = [c for c in display_cols if c in df_sizes.columns]\n",
    "df_sizes.to_csv(out_csv, index=False, columns=cols_to_write)\n",
    "print(f\"Wrote dataset sizes to: {out_csv}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1312065f",
   "metadata": {},
   "source": [
    "## 2) Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d538944e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def _suffix_ok(path: Path):\n",
    "    try:\n",
    "        return path.suffix.lower() in IMG_EXTS\n",
    "    except Exception:\n",
    "        return False\n",
    "\n",
    "def iter_images(root: Path):\n",
    "    # Recursive scan with case-insensitive filter by suffix\n",
    "    files = [Path(p) for p in glob.glob(str(root / '**' / '*'), recursive=True)]\n",
    "    # Sort for deterministic order across different systems/filesystems\n",
    "    return sorted([str(p) for p in files if p.is_file() and _suffix_ok(p)])\n",
    "\n",
    "def safe_open_image(p: Path):\n",
    "    \"\"\"Return (width, height, format) or (None, None, None) on failure.\"\"\"\n",
    "    try:\n",
    "        with Image.open(p) as im:\n",
    "            im.verify()   # quick structural check\n",
    "        with Image.open(p) as im2:\n",
    "            im2.load()    # force decode\n",
    "            return im2.width, im2.height, im2.format\n",
    "    except Exception:\n",
    "        return None, None, None\n",
    "\n",
    "def find_label_for(img_path: Path, dataset_key: str, dataset_root: Path):\n",
    "    \"\"\"Find YOLO .txt label path for an image.\"\"\"\n",
    "    # DeepFish variants: labels next to images\n",
    "    if dataset_key in ['deepfish', 'deepfish_negatives']:\n",
    "        return img_path.with_suffix('.txt')\n",
    "\n",
    "    # Typical YOLO trees\n",
    "    try:\n",
    "        rel = img_path.relative_to(dataset_root)\n",
    "    except Exception:\n",
    "        rel = img_path.name\n",
    "\n",
    "    candidates = []\n",
    "    for labdir in ['labels', 'labels_bbox', 'annotations']:\n",
    "        parts = list(rel.parts) if isinstance(rel, Path) else [rel]\n",
    "        if isinstance(rel, Path) and 'images' in parts:\n",
    "            idx = parts.index('images')\n",
    "            parts[idx] = labdir\n",
    "            label_rel = Path(*parts).with_suffix('.txt')\n",
    "        else:\n",
    "            label_rel = Path(labdir) / (Path(img_path.stem).name + '.txt')\n",
    "        candidates.append(dataset_root / label_rel)\n",
    "\n",
    "    for c in candidates:\n",
    "        if c.exists():\n",
    "            return c\n",
    "    return None\n",
    "\n",
    "def parse_yolo_label(path: Path):\n",
    "    \"\"\"Return (n_lines, n_valid, n_bad, n_oob_or_zero_area).\"\"\"\n",
    "    n_lines = n_valid = n_bad = n_oob_za = 0\n",
    "    if path is None or not path.exists():\n",
    "        return 0, 0, 0, 0\n",
    "    try:\n",
    "        with open(path, 'r') as f:\n",
    "            for line in f:\n",
    "                s = line.strip().split()\n",
    "                if len(s) != 5:\n",
    "                    n_bad += 1; n_lines += 1; continue\n",
    "                try:\n",
    "                    _cid = int(float(s[0])); x, y, w, h = map(float, s[1:])\n",
    "                except Exception:\n",
    "                    n_bad += 1; n_lines += 1; continue\n",
    "                if (0 <= x <= 1) and (0 <= y <= 1) and (0 < w <= 1) and (0 < h <= 1):\n",
    "                    n_valid += 1\n",
    "                else:\n",
    "                    n_oob_za += 1\n",
    "                n_lines += 1\n",
    "    except Exception:\n",
    "        # Treat unreadable label as 1 bad line so it can be flagged upstream if needed\n",
    "        return 1, 0, 1, 0\n",
    "    return n_lines, n_valid, n_bad, n_oob_za\n",
    "\n",
    "def average_hash_hex(img_path: Path, hash_size=8):\n",
    "    try:\n",
    "        with Image.open(img_path) as im:\n",
    "            im = im.convert('L').resize((hash_size, hash_size), Image.BILINEAR)\n",
    "            px = np.asarray(im, dtype=np.float32)\n",
    "            m = px.mean()\n",
    "            bits = (px > m).astype(np.uint8).flatten()\n",
    "            value = 0\n",
    "            for b in bits:\n",
    "                value = (value << 1) | int(b)\n",
    "            return f\"{value:0{hash_size*hash_size//4}x}\"\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def file_md5(path: Path):\n",
    "    try:\n",
    "        h = hashlib.md5()\n",
    "        with open(path, 'rb') as f:\n",
    "            for chunk in iter(lambda: f.read(8192), b''):\n",
    "                h.update(chunk)\n",
    "        return h.hexdigest()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def normalized_label_md5(p: Path):\n",
    "    try:\n",
    "        lines = []\n",
    "        with open(p, 'r') as f:\n",
    "            for line in f:\n",
    "                s = ' '.join(line.strip().split())\n",
    "                if s:\n",
    "                    lines.append(s)\n",
    "        lines = sorted(lines)\n",
    "        blob = '\\n'.join(lines).encode('utf-8')\n",
    "        return hashlib.md5(blob).hexdigest()\n",
    "    except Exception:\n",
    "        return None\n",
    "\n",
    "def is_negatives_subpath(path: Path):\n",
    "    \"\"\"Heuristics to recognize a 'negatives' subfolder name in path parts.\"\"\"\n",
    "    return any('negative' in part.lower() for part in path.parts)\n",
    "\n",
    "def safe_exists(p):\n",
    "    try:\n",
    "        return Path(str(p)).exists()\n",
    "    except Exception:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7c90516",
   "metadata": {},
   "source": [
    "\n",
    "## 3) Audit datasets\n",
    "\n",
    "This step scans each dataset for images, locates YOLO label files, validates label lines, and assigns a `status`.\n",
    "\n",
    "**Statuses**\n",
    "- `ok` — image readable; label either not required (negatives/sparse unlabeled) or has valid boxes\n",
    "- `image_corrupted` — image cannot be opened/decoded\n",
    "- `label_missing_unexpected` — label is expected but missing (not sparse source)\n",
    "- `sparse_unlabeled_frame` — sparse dataset frame without label (excluded from training; not a negative)\n",
    "- `label_all_invalid` — label exists but has no valid boxes\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "4de4f855",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Auditing AquaCoop (1250 files) ...\n",
      "[INFO] Auditing OzFish (350 files) ...\n",
      "[INFO] Auditing aquarium (638 files) ...\n",
      "[INFO] Auditing deepfish (6517 files) ...\n",
      "[INFO] Auditing deepfish_negatives (2012 files) ...\n",
      "[INFO] Auditing f4k (77235 files) ...\n",
      "[INFO] Auditing fish_416 (1350 files) ...\n",
      "[INFO] Auditing fishclef (53196 files) ...\n",
      "[INFO] Auditing luderick (4276 files) ...\n",
      "[OK] Audit complete. Rows = 144812\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>status</th>\n",
       "      <th>image_path</th>\n",
       "      <th>label_path</th>\n",
       "      <th>label_exists</th>\n",
       "      <th>n_label_lines</th>\n",
       "      <th>n_valid_boxes</th>\n",
       "      <th>n_bad_format</th>\n",
       "      <th>n_oob_or_zero_area</th>\n",
       "      <th>img_width</th>\n",
       "      <th>img_height</th>\n",
       "      <th>img_format</th>\n",
       "      <th>expected_labels</th>\n",
       "      <th>sparse_source</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AquaCoop</td>\n",
       "      <td>ok</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>True</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>JPEG</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AquaCoop</td>\n",
       "      <td>ok</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>True</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>JPEG</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AquaCoop</td>\n",
       "      <td>ok</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>True</td>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>JPEG</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AquaCoop</td>\n",
       "      <td>ok</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>True</td>\n",
       "      <td>3</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>JPEG</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AquaCoop</td>\n",
       "      <td>ok</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>True</td>\n",
       "      <td>11</td>\n",
       "      <td>11</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>640.0</td>\n",
       "      <td>JPEG</td>\n",
       "      <td>True</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     source status                                         image_path  \\\n",
       "0  AquaCoop     ok  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...   \n",
       "1  AquaCoop     ok  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...   \n",
       "2  AquaCoop     ok  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...   \n",
       "3  AquaCoop     ok  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...   \n",
       "4  AquaCoop     ok  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...   \n",
       "\n",
       "                                          label_path  label_exists  \\\n",
       "0  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...          True   \n",
       "1  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...          True   \n",
       "2  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...          True   \n",
       "3  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...          True   \n",
       "4  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...          True   \n",
       "\n",
       "   n_label_lines  n_valid_boxes  n_bad_format  n_oob_or_zero_area  img_width  \\\n",
       "0              1              1             0                   0      640.0   \n",
       "1              7              7             0                   0      640.0   \n",
       "2              8              8             0                   0      640.0   \n",
       "3              3              3             0                   0      640.0   \n",
       "4             11             11             0                   0      640.0   \n",
       "\n",
       "   img_height img_format  expected_labels  sparse_source  \n",
       "0       640.0       JPEG             True          False  \n",
       "1       640.0       JPEG             True          False  \n",
       "2       640.0       JPEG             True          False  \n",
       "3       640.0       JPEG             True          False  \n",
       "4       640.0       JPEG             True          False  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "SCHEMA = {\n",
    "    'source': None, 'status': None,\n",
    "    'image_path': None, 'label_path': None,\n",
    "    'label_exists': False,\n",
    "    'n_label_lines': 0, 'n_valid_boxes': 0, 'n_bad_format': 0, 'n_oob_or_zero_area': 0,\n",
    "    'img_width': None, 'img_height': None, 'img_format': None,\n",
    "    'expected_labels': None, 'sparse_source': None\n",
    "}\n",
    "\n",
    "records = []\n",
    "\n",
    "# Process datasets in sorted order for reproducibility\n",
    "for key, sub in sorted(SOURCE_DATASETS.items()):\n",
    "    dataset_root = ROOT / sub\n",
    "    expected = (key != 'deepfish_negatives')\n",
    "    sparse = key in SPARSE_SOURCES\n",
    "\n",
    "    if not dataset_root.exists():\n",
    "        rec = SCHEMA.copy()\n",
    "        rec.update({'source': key, 'status': 'dataset_missing',\n",
    "                    'expected_labels': expected, 'sparse_source': sparse})\n",
    "        records.append(rec)\n",
    "        print(f'[WARN] Dataset missing: {dataset_root}')\n",
    "        continue\n",
    "\n",
    "    imgs = iter_images(dataset_root)\n",
    "    if len(imgs) == 0:\n",
    "        rec = SCHEMA.copy()\n",
    "        rec.update({'source': key, 'status': 'no_images_found',\n",
    "                    'expected_labels': expected, 'sparse_source': sparse})\n",
    "        records.append(rec)\n",
    "        print(f'[WARN] No images found under: {dataset_root}')\n",
    "        continue\n",
    "\n",
    "    print(f'[INFO] Auditing {key} ({len(imgs)} files) ...')\n",
    "    for ip in imgs:\n",
    "        ipath = Path(ip)\n",
    "        # Avoid overlap: don't let 'deepfish' ingest the negatives subfolder\n",
    "        if key == 'deepfish' and is_negatives_subpath(ipath):\n",
    "            continue\n",
    "\n",
    "        w, h, fmt = safe_open_image(ipath)\n",
    "        lp = find_label_for(ipath, key, dataset_root)\n",
    "        label_exists = bool(lp and lp.exists())\n",
    "        n_lines, n_valid, n_bad, n_oob = parse_yolo_label(lp) if label_exists else (0,0,0,0)\n",
    "\n",
    "        status = 'ok'\n",
    "        if w is None:\n",
    "            status = 'image_corrupted'\n",
    "        elif expected and not label_exists and not sparse:\n",
    "            status = 'label_missing_unexpected'\n",
    "        elif expected and sparse and not label_exists:\n",
    "            status = 'sparse_unlabeled_frame'\n",
    "        elif label_exists and n_lines > 0 and n_valid == 0:\n",
    "            status = 'label_all_invalid'\n",
    "\n",
    "        rec = SCHEMA.copy()\n",
    "        rec.update({'source': key, 'status': status,\n",
    "                    'image_path': str(ipath), 'label_path': str(lp) if lp else None,\n",
    "                    'label_exists': label_exists,\n",
    "                    'n_label_lines': n_lines, 'n_valid_boxes': n_valid,\n",
    "                    'n_bad_format': n_bad, 'n_oob_or_zero_area': n_oob,\n",
    "                    'img_width': w, 'img_height': h, 'img_format': fmt,\n",
    "                    'expected_labels': expected, 'sparse_source': sparse})\n",
    "        records.append(rec)\n",
    "\n",
    "audit_df = pd.DataFrame.from_records(records, columns=list(SCHEMA.keys()))\n",
    "print(f\"[OK] Audit complete. Rows = {len(audit_df)}\")\n",
    "display(audit_df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ac23c5c",
   "metadata": {},
   "source": [
    "\n",
    "## 4) Deduplication (avoid overlap, dedup by path and content)\n",
    "\n",
    "We now build a consolidated list of images across sources, avoid the negatives overlap in `deepfish`,\n",
    "and deduplicate by:\n",
    "- **Absolute path** using a preferred source order\n",
    "- **Content** using **aHash + MD5** (exact duplicates)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "09a3984e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Computing image hashes (aHash + MD5) ...\n",
      "[INFO] Exact duplicate image rows = 29749\n",
      "[OK] Dedup done. Rows = 129388\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>image_path</th>\n",
       "      <th>label_path</th>\n",
       "      <th>keep_reason</th>\n",
       "      <th>ahash</th>\n",
       "      <th>img_md5</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AquaCoop</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>unique_content</td>\n",
       "      <td>ffffff1f00000000</td>\n",
       "      <td>58d7f7a4f7e3ce167487285d15d6cdc8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AquaCoop</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>unique_content</td>\n",
       "      <td>ffffff7c00000000</td>\n",
       "      <td>29f76e7dce1d0408075e6579b7159880</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AquaCoop</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>unique_content</td>\n",
       "      <td>ffffff7f00000000</td>\n",
       "      <td>fce35dcc67b6519f6f313916e8e3192a</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AquaCoop</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>unique_content</td>\n",
       "      <td>ffffffff00000000</td>\n",
       "      <td>0ef4734bb5cfb59f5df9d576cb918cfb</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AquaCoop</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>unique_content</td>\n",
       "      <td>ffffff1f00000000</td>\n",
       "      <td>39096b6380bab777cf6708b19a521434</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     source                                         image_path  \\\n",
       "0  AquaCoop  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...   \n",
       "1  AquaCoop  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...   \n",
       "2  AquaCoop  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...   \n",
       "3  AquaCoop  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...   \n",
       "4  AquaCoop  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...   \n",
       "\n",
       "                                          label_path     keep_reason  \\\n",
       "0  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...  unique_content   \n",
       "1  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...  unique_content   \n",
       "2  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...  unique_content   \n",
       "3  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...  unique_content   \n",
       "4  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...  unique_content   \n",
       "\n",
       "              ahash                           img_md5  \n",
       "0  ffffff1f00000000  58d7f7a4f7e3ce167487285d15d6cdc8  \n",
       "1  ffffff7c00000000  29f76e7dce1d0408075e6579b7159880  \n",
       "2  ffffff7f00000000  fce35dcc67b6519f6f313916e8e3192a  \n",
       "3  ffffffff00000000  0ef4734bb5cfb59f5df9d576cb918cfb  \n",
       "4  ffffff1f00000000  39096b6380bab777cf6708b19a521434  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Build scan list\n",
    "rows = []\n",
    "# Process datasets in sorted order for reproducibility\n",
    "for key, sub in sorted(SOURCE_DATASETS.items()):\n",
    "    dataset_root = ROOT / sub\n",
    "    if not dataset_root.exists():\n",
    "        continue\n",
    "    imgs = iter_images(dataset_root)\n",
    "    for ip in imgs:\n",
    "        ipath = Path(ip)\n",
    "        # Avoid overlap from DeepFish: skip negatives under the main deepfish key\n",
    "        if key == 'deepfish' and is_negatives_subpath(ipath):\n",
    "            continue\n",
    "        lp = find_label_for(ipath, key, dataset_root)\n",
    "        rows.append({'source': key, 'image_path': str(ipath), 'label_path': str(lp) if lp else None})\n",
    "\n",
    "scan_df = pd.DataFrame(rows)\n",
    "if scan_df.empty:\n",
    "    raise SystemExit(\"No images found. Check ROOT/SOURCE_DATASETS paths.\")\n",
    "\n",
    "# Keep only existing image files\n",
    "scan_df['image_path'] = scan_df['image_path'].astype(str)\n",
    "scan_df = scan_df[scan_df['image_path'].apply(lambda p: Path(p).exists())].copy()\n",
    "\n",
    "# (A) Deduplicate by absolute path with preferred source order\n",
    "pr_map = {s: i for i, s in enumerate(PREFERRED_SOURCE_ORDER)}\n",
    "scan_df['source_priority'] = scan_df['source'].map(pr_map).fillna(len(PREFERRED_SOURCE_ORDER)).astype(int)\n",
    "\n",
    "dedup_by_path = (\n",
    "    scan_df.sort_values(['image_path', 'source_priority'])\n",
    "           .drop_duplicates(subset=['image_path'], keep='first')\n",
    "           .drop(columns=['source_priority'])\n",
    "           .copy()\n",
    ")\n",
    "dedup_by_path['keep_reason'] = 'unique_or_preferred_source_by_path'\n",
    "\n",
    "# (B) Deduplicate by content: compute hashes\n",
    "print('[INFO] Computing image hashes (aHash + MD5) ...')\n",
    "dedup_by_path['ahash']   = dedup_by_path['image_path'].apply(lambda p: average_hash_hex(Path(p)))\n",
    "dedup_by_path['img_md5'] = dedup_by_path['image_path'].apply(lambda p: file_md5(Path(p)))\n",
    "\n",
    "# Identify exact duplicate content clusters\n",
    "dups_mask = dedup_by_path['ahash'].notna() & dedup_by_path['img_md5'].notna()\n",
    "img_dups = (\n",
    "    dedup_by_path[dups_mask]\n",
    "    .groupby(['ahash','img_md5'], dropna=True)\n",
    "    .filter(lambda g: len(g) > 1)\n",
    "    .sort_values(['ahash','img_md5','source','image_path'])\n",
    ")\n",
    "\n",
    "print(f\"[INFO] Exact duplicate image rows = {len(img_dups)}\" )\n",
    "\n",
    "def _priority(row):\n",
    "    return (pr_map.get(row.get('source'), len(PREFERRED_SOURCE_ORDER)),\n",
    "            len(str(row.get('image_path',''))),\n",
    "            str(row.get('image_path','')))\n",
    "\n",
    "keep_rows = []\n",
    "# Process groups in sorted order for reproducibility\n",
    "for (ah, md5), g in sorted(dedup_by_path.groupby(['ahash','img_md5'], dropna=True)):\n",
    "    g = g.copy()\n",
    "    if len(g) == 1:\n",
    "        g['keep_reason'] = 'unique_content'\n",
    "        keep_rows.append(g.iloc[0])\n",
    "    else:\n",
    "        best_idx = min(g.index, key=lambda i: _priority(g.loc[i]))\n",
    "        g.loc[best_idx, 'keep_reason'] = f'content_cluster_keeper:{str(ah)[:6]}::{str(md5)[:8]}'\n",
    "        keep_rows.append(g.loc[best_idx])\n",
    "\n",
    "# Rows with missing hashes (keep as-is, but mark reason)\n",
    "nan_hash = dedup_by_path[dedup_by_path['ahash'].isna() | dedup_by_path['img_md5'].isna()].copy()\n",
    "if not nan_hash.empty:\n",
    "    nan_hash['keep_reason'] = 'hash_missing_keep'\n",
    "\n",
    "final_keep_dedup = pd.concat([pd.DataFrame(keep_rows), nan_hash], ignore_index=True)\n",
    "final_keep_dedup = final_keep_dedup.drop_duplicates(subset=['image_path']).sort_values(['source','image_path']).reset_index(drop=True)\n",
    "\n",
    "print(f\"[OK] Dedup done. Rows = {len(final_keep_dedup)}\")\n",
    "display(final_keep_dedup.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c907209",
   "metadata": {},
   "source": [
    "\n",
    "## 5) Build **supervised-ready** keep/remove lists\n",
    "\n",
    "Merge with audit statuses and filter:\n",
    "- **Keep**:\n",
    "  - Positives: `status == \"ok\"` and `n_valid_boxes > 0`\n",
    "  - Negatives: images from the designated **negatives** dataset (`deepfish_negatives`) and (optionally) from `negatives_kept.csv`\n",
    "- **Remove**: everything else, including **bad-for-training**:\n",
    "  - `sparse_unlabeled_frame`, `image_corrupted`, `label_missing_unexpected`, `label_all_invalid`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "29d7d3f6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[OK] Wrote: /Users/Marco/Desktop/SmartFISHER/SmartFISHER_DATASET/GitHub_Test/pictures_keep.csv\n",
      "[OK] Wrote: /Users/Marco/Desktop/SmartFISHER/SmartFISHER_DATASET/GitHub_Test/pictures_remove.csv\n",
      "\n",
      "Summary:\n",
      " Positives kept: 26753\n",
      " Negatives kept: 2012\n",
      " Excluded (bad statuses): 100622\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>image_path</th>\n",
       "      <th>label_path</th>\n",
       "      <th>keep_reason</th>\n",
       "      <th>ahash</th>\n",
       "      <th>img_md5</th>\n",
       "      <th>status</th>\n",
       "      <th>n_valid_boxes</th>\n",
       "      <th>expected_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>AquaCoop</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>unique_content</td>\n",
       "      <td>ffffff1f00000000</td>\n",
       "      <td>58d7f7a4f7e3ce167487285d15d6cdc8</td>\n",
       "      <td>ok</td>\n",
       "      <td>1</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>AquaCoop</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>unique_content</td>\n",
       "      <td>ffffff7c00000000</td>\n",
       "      <td>29f76e7dce1d0408075e6579b7159880</td>\n",
       "      <td>ok</td>\n",
       "      <td>7</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>AquaCoop</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>unique_content</td>\n",
       "      <td>ffffff7f00000000</td>\n",
       "      <td>fce35dcc67b6519f6f313916e8e3192a</td>\n",
       "      <td>ok</td>\n",
       "      <td>8</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>AquaCoop</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>unique_content</td>\n",
       "      <td>ffffffff00000000</td>\n",
       "      <td>0ef4734bb5cfb59f5df9d576cb918cfb</td>\n",
       "      <td>ok</td>\n",
       "      <td>3</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>AquaCoop</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>unique_content</td>\n",
       "      <td>ffffff1f00000000</td>\n",
       "      <td>39096b6380bab777cf6708b19a521434</td>\n",
       "      <td>ok</td>\n",
       "      <td>11</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     source                                         image_path  \\\n",
       "0  AquaCoop  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...   \n",
       "1  AquaCoop  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...   \n",
       "2  AquaCoop  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...   \n",
       "3  AquaCoop  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...   \n",
       "4  AquaCoop  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...   \n",
       "\n",
       "                                          label_path     keep_reason  \\\n",
       "0  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...  unique_content   \n",
       "1  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...  unique_content   \n",
       "2  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...  unique_content   \n",
       "3  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...  unique_content   \n",
       "4  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...  unique_content   \n",
       "\n",
       "              ahash                           img_md5 status  n_valid_boxes  \\\n",
       "0  ffffff1f00000000  58d7f7a4f7e3ce167487285d15d6cdc8     ok              1   \n",
       "1  ffffff7c00000000  29f76e7dce1d0408075e6579b7159880     ok              7   \n",
       "2  ffffff7f00000000  fce35dcc67b6519f6f313916e8e3192a     ok              8   \n",
       "3  ffffffff00000000  0ef4734bb5cfb59f5df9d576cb918cfb     ok              3   \n",
       "4  ffffff1f00000000  39096b6380bab777cf6708b19a521434     ok             11   \n",
       "\n",
       "   expected_labels  \n",
       "0             True  \n",
       "1             True  \n",
       "2             True  \n",
       "3             True  \n",
       "4             True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>source</th>\n",
       "      <th>image_path</th>\n",
       "      <th>label_path</th>\n",
       "      <th>keep_reason</th>\n",
       "      <th>ahash</th>\n",
       "      <th>img_md5</th>\n",
       "      <th>status</th>\n",
       "      <th>n_valid_boxes</th>\n",
       "      <th>expected_labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f4k</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>None</td>\n",
       "      <td>content_cluster_keeper:7f7f3f::35cb51d8</td>\n",
       "      <td>7f7f3f3e1e0e0c08</td>\n",
       "      <td>35cb51d82f971b5cd276dc8f34d2e4d3</td>\n",
       "      <td>sparse_unlabeled_frame</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>f4k</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>None</td>\n",
       "      <td>unique_content</td>\n",
       "      <td>7f7f3f3e1e0e0c08</td>\n",
       "      <td>f54e08b0d2c475b64e40d476adaf2451</td>\n",
       "      <td>sparse_unlabeled_frame</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>f4k</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>None</td>\n",
       "      <td>content_cluster_keeper:7f7f3f::408fb950</td>\n",
       "      <td>7f7f3f3f1e0e0c08</td>\n",
       "      <td>408fb9505db9bf0cb8b9a7129b6861f7</td>\n",
       "      <td>sparse_unlabeled_frame</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>f4k</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>None</td>\n",
       "      <td>unique_content</td>\n",
       "      <td>7f7f3f1e1e0e0828</td>\n",
       "      <td>ff5c8dddd0486fa11f7e376d0aa61d91</td>\n",
       "      <td>sparse_unlabeled_frame</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>f4k</td>\n",
       "      <td>/Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...</td>\n",
       "      <td>None</td>\n",
       "      <td>content_cluster_keeper:7f7f3f::f58ef3dd</td>\n",
       "      <td>7f7f3f1e1e0e0808</td>\n",
       "      <td>f58ef3dd3fad46b3309506d51c8f95af</td>\n",
       "      <td>sparse_unlabeled_frame</td>\n",
       "      <td>0</td>\n",
       "      <td>True</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  source                                         image_path label_path  \\\n",
       "0    f4k  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...       None   \n",
       "1    f4k  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...       None   \n",
       "2    f4k  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...       None   \n",
       "3    f4k  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...       None   \n",
       "4    f4k  /Users/Marco/Desktop/SmartFISHER/SmartFISHER_D...       None   \n",
       "\n",
       "                               keep_reason             ahash  \\\n",
       "0  content_cluster_keeper:7f7f3f::35cb51d8  7f7f3f3e1e0e0c08   \n",
       "1                           unique_content  7f7f3f3e1e0e0c08   \n",
       "2  content_cluster_keeper:7f7f3f::408fb950  7f7f3f3f1e0e0c08   \n",
       "3                           unique_content  7f7f3f1e1e0e0828   \n",
       "4  content_cluster_keeper:7f7f3f::f58ef3dd  7f7f3f1e1e0e0808   \n",
       "\n",
       "                            img_md5                  status  n_valid_boxes  \\\n",
       "0  35cb51d82f971b5cd276dc8f34d2e4d3  sparse_unlabeled_frame              0   \n",
       "1  f54e08b0d2c475b64e40d476adaf2451  sparse_unlabeled_frame              0   \n",
       "2  408fb9505db9bf0cb8b9a7129b6861f7  sparse_unlabeled_frame              0   \n",
       "3  ff5c8dddd0486fa11f7e376d0aa61d91  sparse_unlabeled_frame              0   \n",
       "4  f58ef3dd3fad46b3309506d51c8f95af  sparse_unlabeled_frame              0   \n",
       "\n",
       "   expected_labels  \n",
       "0             True  \n",
       "1             True  \n",
       "2             True  \n",
       "3             True  \n",
       "4             True  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "# Join deduped list to audit for statuses and label counts\n",
    "audit_df['image_path'] = audit_df['image_path'].astype(str)\n",
    "final_keep_dedup['image_path'] = final_keep_dedup['image_path'].astype(str)\n",
    "\n",
    "fk_aud = final_keep_dedup.merge(\n",
    "    audit_df[['image_path','source','status','n_valid_boxes','expected_labels']],\n",
    "    on=['image_path','source'],\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "bad_status = {'sparse_unlabeled_frame','image_corrupted','label_missing_unexpected','label_all_invalid'}\n",
    "\n",
    "# Positives (with valid boxes)\n",
    "pos_mask = (fk_aud['status'] == 'ok') & (fk_aud.get('n_valid_boxes', 0).fillna(0) > 0)\n",
    "\n",
    "# Negatives: from explicit negatives dataset + optionally reviewed negatives list\n",
    "neg_mask = (fk_aud['source'] == 'deepfish_negatives')\n",
    "if REVIEWED_NEG_CSV.exists():\n",
    "    kept_negs = pd.read_csv(REVIEWED_NEG_CSV)\n",
    "    kept_negs['image_path'] = kept_negs['image_path'].astype(str)\n",
    "    fk_aud['is_kept_review_neg'] = fk_aud['image_path'].isin(kept_negs['image_path'])\n",
    "    neg_mask = neg_mask | fk_aud['is_kept_review_neg']\n",
    "\n",
    "not_bad = ~fk_aud['status'].isin(bad_status)\n",
    "\n",
    "supervised_keep = fk_aud[(not_bad) & (pos_mask | neg_mask)].copy()\n",
    "supervised_keep = supervised_keep.reset_index(drop=True)\n",
    "\n",
    "# Everything else is remove\n",
    "supervised_remove = fk_aud[~fk_aud.index.isin(supervised_keep.index)].copy().reset_index(drop=True)\n",
    "\n",
    "# Save exactly two CSV files (as requested)\n",
    "# Ensure export directory exists before saving CSV files\n",
    "EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "(supervised_keep[['source','image_path','label_path','status','n_valid_boxes','keep_reason']]\n",
    " .sort_values(['source','image_path'])\n",
    " .to_csv(PICTURES_KEEP_CSV, index=False))\n",
    "\n",
    "(supervised_remove[['source','image_path','label_path','status','n_valid_boxes','keep_reason']]\n",
    " .sort_values(['source','image_path'])\n",
    " .to_csv(PICTURES_REMOVE_CSV, index=False))\n",
    "\n",
    "# User-facing summary\n",
    "pos_kept = int(((supervised_keep['status'] == 'ok') & (supervised_keep.get('n_valid_boxes', 0).fillna(0) > 0)).sum())\n",
    "neg_kept = int((supervised_keep['source'] == 'deepfish_negatives').sum() + supervised_keep.get('is_kept_review_neg', pd.Series(False, index=supervised_keep.index)).sum())\n",
    "excluded_bad = int((fk_aud['status'].isin(bad_status)).sum())\n",
    "\n",
    "print('[OK] Wrote:', PICTURES_KEEP_CSV)\n",
    "print('[OK] Wrote:', PICTURES_REMOVE_CSV)\n",
    "print('\\nSummary:')\n",
    "print(' Positives kept:', pos_kept)\n",
    "print(' Negatives kept:', neg_kept)\n",
    "print(' Excluded (bad statuses):', excluded_bad)\n",
    "\n",
    "display(supervised_keep.head())\n",
    "display(supervised_remove.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8db41b8c",
   "metadata": {},
   "source": [
    "## 6) Export **supervised-ready** dataset (images + labels to keep)\n",
    "\n",
    "This will create the folder defined in `EXPORT_DIR` and copy:\n",
    "- All **kept** images organized by dataset with sequential naming (datasetname_00001.jpg, etc.)\n",
    "- Their YOLO **labels** when available (for negatives, labels may not exist by design)\n",
    "- **data.yaml** files from each dataset to preserve class definitions and configurations\n",
    "- **Negative datasets** are placed under `negatives/` subfolder structure\n",
    "- A **consolidated data.yaml** file is created at the root level combining all class information\n",
    "\n",
    "**Export structure:**\n",
    "```\n",
    "supervised_ready_dataset/\n",
    "├── data.yaml                           # Consolidated dataset config\n",
    "├── luderick/\n",
    "│   ├── images/\n",
    "│   │   ├── luderick_00001.jpg\n",
    "│   │   └── luderick_00002.jpg\n",
    "│   ├── labels/\n",
    "│   │   ├── luderick_00001.txt\n",
    "│   │   └── luderick_00002.txt\n",
    "│   └── data.yaml                       # Original dataset config\n",
    "├── fish_416/\n",
    "│   ├── images/\n",
    "│   │   ├── fish_416_00001.jpg\n",
    "│   │   └── fish_416_00002.jpg\n",
    "│   ├── labels/\n",
    "│   │   ├── fish_416_00001.txt\n",
    "│   │   └── fish_416_00002.txt\n",
    "│   └── data.yaml                       # Original dataset config\n",
    "└── negatives/\n",
    "    └── deepfish_negatives/\n",
    "        ├── images/\n",
    "        │   ├── deepfish_negatives_00001.jpg\n",
    "        │   └── deepfish_negatives_00002.jpg\n",
    "        ├── labels/\n",
    "        │   └── deepfish_negatives_00001.txt\n",
    "        └── data.yaml                   # Original dataset config (if exists)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "72f29fd2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Export directory exists: /Users/Marco/Desktop/SmartFISHER/SmartFISHER_DATASET/GitHub_Test.\n",
      "       Files may be overwritten.\n",
      "[INFO] Copied data.yaml for AquaCoop\n",
      "[INFO] Processing AquaCoop: 1238 files -> /Users/Marco/Desktop/SmartFISHER/SmartFISHER_DATASET/GitHub_Test/AquaCoop\n",
      "[INFO] Copied data.yaml for OzFish\n",
      "[INFO] Processing OzFish: 350 files -> /Users/Marco/Desktop/SmartFISHER/SmartFISHER_DATASET/GitHub_Test/OzFish\n",
      "[INFO] Copied data.yaml for aquarium\n",
      "[INFO] Processing aquarium: 637 files -> /Users/Marco/Desktop/SmartFISHER/SmartFISHER_DATASET/GitHub_Test/aquarium\n",
      "[INFO] Created data.yaml for deepfish from classes.txt (1 classes)\n",
      "[INFO] Processing deepfish: 4505 files -> /Users/Marco/Desktop/SmartFISHER/SmartFISHER_DATASET/GitHub_Test/deepfish\n",
      "[INFO] Processing deepfish_negatives: 2012 files -> /Users/Marco/Desktop/SmartFISHER/SmartFISHER_DATASET/GitHub_Test/negatives/deepfish_negatives\n",
      "[INFO] Processing f4k: 794 files -> /Users/Marco/Desktop/SmartFISHER/SmartFISHER_DATASET/GitHub_Test/f4k\n",
      "[INFO] Copied data.yaml for fish_416\n",
      "[INFO] Processing fish_416: 680 files -> /Users/Marco/Desktop/SmartFISHER/SmartFISHER_DATASET/GitHub_Test/fish_416\n",
      "[INFO] Processing fishclef: 14273 files -> /Users/Marco/Desktop/SmartFISHER/SmartFISHER_DATASET/GitHub_Test/fishclef\n",
      "[INFO] Copied data.yaml for luderick\n",
      "[INFO] Processing luderick: 4276 files -> /Users/Marco/Desktop/SmartFISHER/SmartFISHER_DATASET/GitHub_Test/luderick\n",
      "[INFO] Created consolidated data.yaml with 38 classes\n",
      "\n",
      "[SUCCESS] Export completed successfully!\n",
      "  Images copied: 28765\n",
      "  Labels copied: 28765\n",
      "  YAML configs:  7\n",
      "  Dataset file counts: {'AquaCoop': 1238, 'OzFish': 350, 'aquarium': 637, 'deepfish': 4505, 'deepfish_negatives': 2012, 'f4k': 794, 'fish_416': 680, 'fishclef': 14273, 'luderick': 4276}\n",
      "  Export location: /Users/Marco/Desktop/SmartFISHER/SmartFISHER_DATASET/GitHub_Test\n"
     ]
    }
   ],
   "source": [
    "# =============================================================================\n",
    "# Export Dataset: Create per-dataset folders with images/, labels/, data.yaml\n",
    "# =============================================================================\n",
    "\n",
    "# Initialize export directory\n",
    "EXPORT_DIR.mkdir(parents=True, exist_ok=True)\n",
    "if any(EXPORT_DIR.iterdir()):\n",
    "    print(f'[INFO] Export directory exists: {EXPORT_DIR}.\\n'\n",
    "          f'       Files may be overwritten.')\n",
    "\n",
    "def copy_file_safe(src: Path, dst: Path) -> bool:\n",
    "    \"\"\"Safely copy file with error handling and directory creation.\"\"\"\n",
    "    try:\n",
    "        dst.parent.mkdir(parents=True, exist_ok=True)\n",
    "        shutil.copy2(src, dst)\n",
    "        return True\n",
    "    except Exception as e:\n",
    "        print(f'[WARN] Failed to copy {src} -> {dst}: {e}')\n",
    "        return False\n",
    "\n",
    "def copy_dataset_yaml(source: str, base_dir: Path) -> bool:\n",
    "    \"\"\"Copy or create dataset YAML file. For Deepfish, convert .txt classes to YAML.\"\"\"\n",
    "    original_root = ROOT / SOURCE_DATASETS.get(source, '')\n",
    "\n",
    "    # Try to find existing YAML files\n",
    "    yaml_candidates = ['data.yaml', 'data.yml', 'dataset.yaml', 'dataset.yml']\n",
    "    for yaml_name in yaml_candidates:\n",
    "        yaml_path = original_root / yaml_name\n",
    "        if yaml_path.exists():\n",
    "            dst_path = base_dir / 'data.yaml'  # Standardize to data.yaml\n",
    "            if copy_file_safe(yaml_path, dst_path):\n",
    "                print(f'[INFO] Copied {yaml_name} for {source}')\n",
    "                return True\n",
    "            # If copy failed, try next candidate instead of breaking\n",
    "            continue\n",
    "\n",
    "    # Special handling for Deepfish datasets - look for .txt files with class names\n",
    "    if 'deepfish' in source.lower():\n",
    "        # Common class file names in Deepfish datasets\n",
    "        class_file_candidates = [\n",
    "            'classes.txt', 'class_names.txt', 'labels.txt',\n",
    "            'object_classes.txt', 'categories.txt'\n",
    "        ]\n",
    "        for class_filename in class_file_candidates:\n",
    "            class_file_path = original_root / class_filename\n",
    "            if class_file_path.exists():\n",
    "                try:\n",
    "                    # Read class names from .txt file\n",
    "                    with open(class_file_path, 'r', encoding='utf-8') as f:\n",
    "                        class_names = [line.strip() for line in f if line.strip()]\n",
    "\n",
    "                    if class_names:\n",
    "                        # Create data.yaml content\n",
    "                        yaml_lines = [\n",
    "                            f\"# Dataset configuration for {source}\",\n",
    "                            f\"# Generated from {class_filename}\",\n",
    "                            f\"path: {base_dir}\",\n",
    "                            \"train: images\",\n",
    "                            \"val: images  # Configure train/val split as needed\",\n",
    "                            \"test: ''\",\n",
    "                            \"\",\n",
    "                            \"# Class names (adjust IDs if needed)\",\n",
    "                            \"names:\",\n",
    "                        ]\n",
    "                        for i, class_name in enumerate(class_names):\n",
    "                            yaml_lines.append(f\"  {i}: {class_name}\")\n",
    "                        yaml_lines.append(f\"\\nnc: {len(class_names)}  # Number of classes\\n\")\n",
    "\n",
    "                        dst_path = base_dir / 'data.yaml'\n",
    "                        dst_path.parent.mkdir(parents=True, exist_ok=True)\n",
    "                        with open(dst_path, 'w', encoding='utf-8') as f:\n",
    "                            f.write('\\n'.join(yaml_lines))\n",
    "\n",
    "                        print(f'[INFO] Created data.yaml for {source} from {class_filename} '\n",
    "                              f'({len(class_names)} classes)')\n",
    "                        return True\n",
    "\n",
    "                except Exception as e:\n",
    "                    print(f'[WARN] Could not process {class_file_path}: {e}')\n",
    "                    continue\n",
    "\n",
    "    return False\n",
    "\n",
    "# Process each unique dataset source\n",
    "stats = {'images': 0, 'labels': 0, 'yamls': 0}\n",
    "dataset_counters = {}\n",
    "\n",
    "for source in supervised_keep['source'].unique():\n",
    "    dataset_rows = supervised_keep[supervised_keep['source'] == source].copy()\n",
    "    dataset_counters[source] = 0\n",
    "\n",
    "    # Determine dataset type and create appropriate directory structure\n",
    "    is_negative = (\n",
    "        source == 'deepfish_negatives'\n",
    "        or dataset_rows.get('is_kept_review_neg',\n",
    "                            pd.Series(False, index=dataset_rows.index)).any()\n",
    "    )\n",
    "\n",
    "    # Create base directory path (negatives go under negatives/ subfolder)\n",
    "    base_dir = EXPORT_DIR / ('negatives' / Path(source) if is_negative else Path(source))\n",
    "    images_dir = base_dir / 'images'\n",
    "    labels_dir = base_dir / 'labels'\n",
    "\n",
    "    # Create directory structure\n",
    "    images_dir.mkdir(parents=True, exist_ok=True)\n",
    "    labels_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "    # Copy dataset configuration file (best-effort)\n",
    "    if copy_dataset_yaml(source, base_dir):\n",
    "        stats['yamls'] += 1\n",
    "\n",
    "    print(f'[INFO] Processing {source}: {len(dataset_rows)} files -> {base_dir}')\n",
    "\n",
    "    # Process all files for this dataset\n",
    "    for _, row in dataset_rows.iterrows():\n",
    "        image_path = Path(row['image_path'])\n",
    "        if not image_path.exists():\n",
    "            continue\n",
    "\n",
    "        # Generate sequential filename\n",
    "        dataset_counters[source] += 1\n",
    "        file_number = f\"{dataset_counters[source]:05d}\"\n",
    "        img_ext = image_path.suffix.lower()\n",
    "\n",
    "        new_img_name = f\"{source}_{file_number}{img_ext}\"\n",
    "        new_lbl_name = f\"{source}_{file_number}.txt\"\n",
    "\n",
    "        # Copy image file\n",
    "        if copy_file_safe(image_path, images_dir / new_img_name):\n",
    "            stats['images'] += 1\n",
    "\n",
    "        # Copy label file if it exists\n",
    "        label_path = row.get('label_path')\n",
    "        if isinstance(label_path, str) and label_path.strip():\n",
    "            label_file = Path(label_path)\n",
    "            if label_file.exists():\n",
    "                if copy_file_safe(label_file, labels_dir / new_lbl_name):\n",
    "                    stats['labels'] += 1\n",
    "\n",
    "# Create consolidated dataset configuration\n",
    "def create_consolidated_yaml() -> bool:\n",
    "    \"\"\"Create a consolidated data.yaml combining all dataset classes.\"\"\"\n",
    "    try:\n",
    "        import yaml\n",
    "\n",
    "        consolidated_config = {\n",
    "            'path': str(EXPORT_DIR),\n",
    "            'train': '',  # To be configured by user based on their training setup\n",
    "            'val': '',    # To be configured by user based on their validation setup\n",
    "            'test': '',   # Optional test set path\n",
    "            'names': {},\n",
    "            'nc': 0,\n",
    "        }\n",
    "\n",
    "        # Collect class information from all datasets (excluding negatives)\n",
    "        all_classes = {}\n",
    "        class_counter = 0\n",
    "\n",
    "        for source in supervised_keep['source'].unique():\n",
    "            if source == 'deepfish_negatives':\n",
    "                continue\n",
    "\n",
    "            # Figure out where the exported YAML would be\n",
    "            is_negative = (\n",
    "                source == 'deepfish_negatives'\n",
    "                or supervised_keep[supervised_keep['source'] == source]\n",
    "                   .get('is_kept_review_neg', pd.Series(False)).any()\n",
    "            )\n",
    "            export_yaml_path = (\n",
    "                EXPORT_DIR / 'negatives' / source / 'data.yaml'\n",
    "                if is_negative else EXPORT_DIR / source / 'data.yaml'\n",
    "            )\n",
    "\n",
    "            # Try exported data.yaml first, then fall back to original dataset\n",
    "            yaml_sources = [export_yaml_path]\n",
    "            dataset_root = ROOT / SOURCE_DATASETS.get(source, '')\n",
    "            for yaml_name in ['data.yaml', 'data.yml', 'dataset.yaml', 'dataset.yml']:\n",
    "                yaml_sources.append(dataset_root / yaml_name)\n",
    "\n",
    "            for yaml_path in yaml_sources:\n",
    "                if yaml_path.exists():\n",
    "                    try:\n",
    "                        with open(yaml_path, 'r', encoding='utf-8') as f:\n",
    "                            dataset_config = yaml.safe_load(f) or {}\n",
    "                        names = dataset_config.get('names')\n",
    "                        if names:\n",
    "                            # Handle both dict and list formats\n",
    "                            if isinstance(names, dict):\n",
    "                                class_list = list(names.values())\n",
    "                            else:\n",
    "                                class_list = list(names)\n",
    "                            for class_name in class_list:\n",
    "                                # Prefix with source to avoid conflicts\n",
    "                                prefixed_name = f\"{source}_{class_name}\"\n",
    "                                all_classes[class_counter] = prefixed_name\n",
    "                                class_counter += 1\n",
    "                        break\n",
    "                    except Exception as e:\n",
    "                        print(f'[WARN] Could not parse {yaml_path}: {e}')\n",
    "                        continue\n",
    "\n",
    "        # Use default class if no classes found\n",
    "        if not all_classes:\n",
    "            all_classes = {0: 'fish'}\n",
    "\n",
    "        consolidated_config['names'] = all_classes\n",
    "        consolidated_config['nc'] = len(all_classes)\n",
    "\n",
    "        # Write consolidated configuration\n",
    "        consolidated_path = EXPORT_DIR / 'data.yaml'\n",
    "        with open(consolidated_path, 'w', encoding='utf-8') as f:\n",
    "            yaml.dump(consolidated_config, f, default_flow_style=False, sort_keys=False)\n",
    "\n",
    "        print(f'[INFO] Created consolidated data.yaml with {len(all_classes)} classes')\n",
    "        return True\n",
    "\n",
    "    except ImportError:\n",
    "        print('[WARN] PyYAML not available - install with: pip install pyyaml')\n",
    "        return False\n",
    "    except Exception as e:\n",
    "        print(f'[WARN] Could not create consolidated data.yaml: {e}')\n",
    "        return False\n",
    "\n",
    "# Generate consolidated configuration\n",
    "if create_consolidated_yaml():\n",
    "    stats['yamls'] += 1\n",
    "\n",
    "# Final summary\n",
    "print('\\n[SUCCESS] Export completed successfully!')\n",
    "print(f'  Images copied: {stats[\"images\"]}')\n",
    "print(f'  Labels copied: {stats[\"labels\"]}')\n",
    "print(f'  YAML configs:  {stats[\"yamls\"]}')\n",
    "print(f'  Dataset file counts: {dict(dataset_counters)}')\n",
    "print(f'  Export location: {EXPORT_DIR}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8da3743e",
   "metadata": {},
   "source": [
    "## 7) Notes & Tips\n",
    "\n",
    "**Configuration:**\n",
    "- **Adjust `SOURCE_DATASETS`**: Ensure all dataset paths are correct and the negatives dataset key matches your structure\n",
    "- **Sparse sources** (`SPARSE_SOURCES`): Unlabeled frames are excluded from training (not treated as negatives)\n",
    "\n",
    "**Label discovery:**\n",
    "- For non-standard layouts, extend the search paths in `find_label_for()` function\n",
    "- DeepFish variants expect labels next to images; other datasets use typical YOLO structure\n",
    "\n",
    "**Deduplication strategy:**\n",
    "- **Path-based**: Uses preferred source order for identical file paths\n",
    "- **Content-based**: Uses aHash + MD5 for exact duplicate detection\n",
    "- For near-duplicates, consider adding Hamming distance threshold on aHash\n",
    "\n",
    "**Export structure:**\n",
    "- **Per-dataset organization**: Each dataset gets its own folder with `images/`, `labels/`, and `data.yaml`\n",
    "- **Sequential naming**: Files renamed as `datasetname_00001.jpg`, `datasetname_00002.jpg`, etc.\n",
    "- **Negatives separation**: Negative datasets placed under `negatives/` subfolder\n",
    "- **Consolidated config**: Root-level `data.yaml` merges all dataset classes with source prefixes\n",
    "\n",
    "**Post-processing recommendations:**\n",
    "- Review the consolidated `data.yaml` and adjust train/val/test paths as needed\n",
    "- Consider splitting datasets into train/validation sets based on your requirements\n",
    "- Verify class mappings and remove source prefixes if desired for cleaner class names"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2a5042a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "923d0782",
   "metadata": {},
   "source": [
    "## 8) Dataset Size Comparison: Before vs After Cleaning\n",
    "\n",
    "This section compares the original dataset sizes (before audit and cleaning) with the final exported dataset sizes (after deduplication, filtering, and export)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "58dfdf7e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "DATASET SIZE COMPARISON: BEFORE vs AFTER CLEANING (PER DATASET)\n",
      "================================================================================\n",
      "\n",
      "📊 PER-DATASET COMPARISON:\n",
      "--------------------------------------------------------------------------------\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>dataset</th>\n",
       "      <th>before_images</th>\n",
       "      <th>before_labels</th>\n",
       "      <th>before_bboxes</th>\n",
       "      <th>after_images</th>\n",
       "      <th>after_labels</th>\n",
       "      <th>after_bboxes</th>\n",
       "      <th>images_removed</th>\n",
       "      <th>labels_removed</th>\n",
       "      <th>bboxes_removed</th>\n",
       "      <th>images_retained_%</th>\n",
       "      <th>labels_retained_%</th>\n",
       "      <th>bboxes_retained_%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>f4k</td>\n",
       "      <td>77235</td>\n",
       "      <td>917</td>\n",
       "      <td>3460</td>\n",
       "      <td>794</td>\n",
       "      <td>794</td>\n",
       "      <td>3054</td>\n",
       "      <td>76441</td>\n",
       "      <td>123</td>\n",
       "      <td>406</td>\n",
       "      <td>1.0</td>\n",
       "      <td>86.6</td>\n",
       "      <td>88.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>fishclef</td>\n",
       "      <td>53196</td>\n",
       "      <td>14809</td>\n",
       "      <td>23294</td>\n",
       "      <td>14273</td>\n",
       "      <td>14273</td>\n",
       "      <td>22627</td>\n",
       "      <td>38923</td>\n",
       "      <td>536</td>\n",
       "      <td>667</td>\n",
       "      <td>26.8</td>\n",
       "      <td>96.4</td>\n",
       "      <td>97.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>deepfish</td>\n",
       "      <td>6517</td>\n",
       "      <td>6518</td>\n",
       "      <td>15464</td>\n",
       "      <td>4505</td>\n",
       "      <td>4505</td>\n",
       "      <td>15463</td>\n",
       "      <td>2012</td>\n",
       "      <td>2013</td>\n",
       "      <td>1</td>\n",
       "      <td>69.1</td>\n",
       "      <td>69.1</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>luderick</td>\n",
       "      <td>4276</td>\n",
       "      <td>8554</td>\n",
       "      <td>18881</td>\n",
       "      <td>4276</td>\n",
       "      <td>4276</td>\n",
       "      <td>9429</td>\n",
       "      <td>0</td>\n",
       "      <td>4278</td>\n",
       "      <td>9452</td>\n",
       "      <td>100.0</td>\n",
       "      <td>50.0</td>\n",
       "      <td>49.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>deepfish_negatives</td>\n",
       "      <td>2012</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>2012</td>\n",
       "      <td>2012</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>fish_416</td>\n",
       "      <td>1350</td>\n",
       "      <td>1352</td>\n",
       "      <td>3183</td>\n",
       "      <td>680</td>\n",
       "      <td>680</td>\n",
       "      <td>1582</td>\n",
       "      <td>670</td>\n",
       "      <td>672</td>\n",
       "      <td>1601</td>\n",
       "      <td>50.4</td>\n",
       "      <td>50.3</td>\n",
       "      <td>49.7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>AquaCoop</td>\n",
       "      <td>1250</td>\n",
       "      <td>1250</td>\n",
       "      <td>13840</td>\n",
       "      <td>1238</td>\n",
       "      <td>1238</td>\n",
       "      <td>13693</td>\n",
       "      <td>12</td>\n",
       "      <td>12</td>\n",
       "      <td>147</td>\n",
       "      <td>99.0</td>\n",
       "      <td>99.0</td>\n",
       "      <td>98.9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>aquarium</td>\n",
       "      <td>638</td>\n",
       "      <td>640</td>\n",
       "      <td>4854</td>\n",
       "      <td>637</td>\n",
       "      <td>637</td>\n",
       "      <td>4821</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>33</td>\n",
       "      <td>99.8</td>\n",
       "      <td>99.5</td>\n",
       "      <td>99.3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>OzFish</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>7540</td>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "      <td>7540</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "      <td>100.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              dataset  before_images  before_labels  before_bboxes  \\\n",
       "0                 f4k          77235            917           3460   \n",
       "1            fishclef          53196          14809          23294   \n",
       "2            deepfish           6517           6518          15464   \n",
       "3            luderick           4276           8554          18881   \n",
       "4  deepfish_negatives           2012           2012              0   \n",
       "5            fish_416           1350           1352           3183   \n",
       "6            AquaCoop           1250           1250          13840   \n",
       "7            aquarium            638            640           4854   \n",
       "8              OzFish            350            350           7540   \n",
       "\n",
       "   after_images  after_labels  after_bboxes  images_removed  labels_removed  \\\n",
       "0           794           794          3054           76441             123   \n",
       "1         14273         14273         22627           38923             536   \n",
       "2          4505          4505         15463            2012            2013   \n",
       "3          4276          4276          9429               0            4278   \n",
       "4          2012          2012             0               0               0   \n",
       "5           680           680          1582             670             672   \n",
       "6          1238          1238         13693              12              12   \n",
       "7           637           637          4821               1               3   \n",
       "8           350           350          7540               0               0   \n",
       "\n",
       "   bboxes_removed  images_retained_%  labels_retained_%  bboxes_retained_%  \n",
       "0             406                1.0               86.6               88.3  \n",
       "1             667               26.8               96.4               97.1  \n",
       "2               1               69.1               69.1              100.0  \n",
       "3            9452              100.0               50.0               49.9  \n",
       "4               0              100.0              100.0                0.0  \n",
       "5            1601               50.4               50.3               49.7  \n",
       "6             147               99.0               99.0               98.9  \n",
       "7              33               99.8               99.5               99.3  \n",
       "8               0              100.0              100.0              100.0  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "📈 OVERALL TOTALS:\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "BEFORE CLEANING:\n",
      "  Images:         146,824\n",
      "  Label Files:    36,402\n",
      "  Bounding Boxes: 90,516\n",
      "\n",
      "AFTER CLEANING:\n",
      "  Images:         28,765\n",
      "  Label Files:    28,765\n",
      "  Bounding Boxes: 78,209\n",
      "\n",
      "REMOVED:\n",
      "  Images:         118,059 (80.4%)\n",
      "  Label Files:    7,637 (21.0%)\n",
      "  Bounding Boxes: 12,307 (13.6%)\n",
      "\n",
      "RETAINED:\n",
      "  Images:         28,765 (19.6%)\n",
      "  Label Files:    28,765 (79.0%)\n",
      "  Bounding Boxes: 78,209 (86.4%)\n",
      "\n",
      "✅ Saved detailed comparison to: /Users/Marco/Desktop/SmartFISHER/SmartFISHER_DATASET/GitHub_Test/dataset_size_comparison_detailed.csv\n",
      "✅ Saved summary to: /Users/Marco/Desktop/SmartFISHER/SmartFISHER_DATASET/GitHub_Test/dataset_size_comparison_summary.csv\n",
      "\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# ================================================================================\n",
    "# Dataset Size Analysis: Before (Original) vs After (Exported/Cleaned)\n",
    "# ================================================================================\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"DATASET SIZE COMPARISON: BEFORE vs AFTER CLEANING (PER DATASET)\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# ---------- BEFORE: Original Dataset Sizes ----------\n",
    "if 'df_sizes' not in globals():\n",
    "    print(\"⚠️  df_sizes not found. Please run cell 2 first.\")\n",
    "else:\n",
    "    df_before = df_sizes[['dataset', 'n_images', 'n_labels', 'n_bboxes']].copy()\n",
    "    df_before.columns = ['dataset', 'before_images', 'before_labels', 'before_bboxes']\n",
    "    \n",
    "    # ---------- AFTER: Exported Dataset Sizes ----------\n",
    "    export_rows = []\n",
    "    export_root = Path(EXPORT_DIR)\n",
    "    \n",
    "    if not export_root.exists():\n",
    "        print(f\"⚠️  Export directory does not exist: {export_root}\")\n",
    "        print(\"Please run cell 6 (Export) first.\")\n",
    "    else:\n",
    "        # Scan exported datasets\n",
    "        for child in sorted(export_root.iterdir()):\n",
    "            if child.name.startswith('.') or child.is_file():\n",
    "                continue\n",
    "                \n",
    "            if child.name == 'negatives':\n",
    "                # Handle negatives subfolder\n",
    "                for neg_dataset in sorted(child.iterdir()):\n",
    "                    if not neg_dataset.is_dir():\n",
    "                        continue\n",
    "                    images_dir = neg_dataset / 'images'\n",
    "                    labels_dir = neg_dataset / 'labels'\n",
    "                    \n",
    "                    n_imgs = len(list(images_dir.glob('*'))) if images_dir.exists() else 0\n",
    "                    n_lbls = len(list(labels_dir.glob('*.txt'))) if labels_dir.exists() else 0\n",
    "                    n_boxes = 0\n",
    "                    \n",
    "                    if labels_dir.exists():\n",
    "                        for lbl_file in labels_dir.glob('*.txt'):\n",
    "                            try:\n",
    "                                with open(lbl_file, 'r', encoding='utf-8') as f:\n",
    "                                    n_boxes += sum(1 for line in f if line.strip())\n",
    "                            except Exception:\n",
    "                                pass\n",
    "                    \n",
    "                    # Map back to original dataset name\n",
    "                    original_name = neg_dataset.name.replace('negatives/', '')\n",
    "                    export_rows.append({\n",
    "                        'dataset': original_name,\n",
    "                        'after_images': n_imgs,\n",
    "                        'after_labels': n_lbls,\n",
    "                        'after_bboxes': n_boxes\n",
    "                    })\n",
    "            else:\n",
    "                # Regular dataset\n",
    "                images_dir = child / 'images'\n",
    "                labels_dir = child / 'labels'\n",
    "                \n",
    "                n_imgs = len(list(images_dir.glob('*'))) if images_dir.exists() else 0\n",
    "                n_lbls = len(list(labels_dir.glob('*.txt'))) if labels_dir.exists() else 0\n",
    "                n_boxes = 0\n",
    "                \n",
    "                if labels_dir.exists():\n",
    "                    for lbl_file in labels_dir.glob('*.txt'):\n",
    "                        try:\n",
    "                            with open(lbl_file, 'r', encoding='utf-8') as f:\n",
    "                                n_boxes += sum(1 for line in f if line.strip())\n",
    "                        except Exception:\n",
    "                            pass\n",
    "                \n",
    "                export_rows.append({\n",
    "                    'dataset': child.name,\n",
    "                    'after_images': n_imgs,\n",
    "                    'after_labels': n_lbls,\n",
    "                    'after_bboxes': n_boxes\n",
    "                })\n",
    "        \n",
    "        df_after = pd.DataFrame(export_rows)\n",
    "        \n",
    "        if not df_after.empty:\n",
    "            # Merge before and after dataframes\n",
    "            df_comparison = df_before.merge(df_after, on='dataset', how='outer').fillna(0)\n",
    "            \n",
    "            # Calculate differences and percentages\n",
    "            df_comparison['images_removed'] = df_comparison['before_images'] - df_comparison['after_images']\n",
    "            df_comparison['labels_removed'] = df_comparison['before_labels'] - df_comparison['after_labels']\n",
    "            df_comparison['bboxes_removed'] = df_comparison['before_bboxes'] - df_comparison['after_bboxes']\n",
    "            \n",
    "            df_comparison['images_retained_%'] = (df_comparison['after_images'] / df_comparison['before_images'] * 100).fillna(0).round(1)\n",
    "            df_comparison['labels_retained_%'] = (df_comparison['after_labels'] / df_comparison['before_labels'] * 100).fillna(0).round(1)\n",
    "            df_comparison['bboxes_retained_%'] = (df_comparison['after_bboxes'] / df_comparison['before_bboxes'] * 100).fillna(0).round(1)\n",
    "            \n",
    "            # Convert to int where appropriate\n",
    "            int_cols = ['before_images', 'before_labels', 'before_bboxes', \n",
    "                       'after_images', 'after_labels', 'after_bboxes',\n",
    "                       'images_removed', 'labels_removed', 'bboxes_removed']\n",
    "            for col in int_cols:\n",
    "                df_comparison[col] = df_comparison[col].astype(int)\n",
    "            \n",
    "            # Sort by before_images descending\n",
    "            df_comparison = df_comparison.sort_values('before_images', ascending=False).reset_index(drop=True)\n",
    "            \n",
    "            # Display detailed comparison\n",
    "            print(\"\\n📊 PER-DATASET COMPARISON:\")\n",
    "            print(\"-\" * 80)\n",
    "            display(df_comparison)\n",
    "            \n",
    "            # ---------- TOTALS SUMMARY ----------\n",
    "            print(\"\\n\" + \"=\" * 80)\n",
    "            print(\"📈 OVERALL TOTALS:\")\n",
    "            print(\"-\" * 80)\n",
    "            \n",
    "            before_total_images = int(df_comparison['before_images'].sum())\n",
    "            before_total_labels = int(df_comparison['before_labels'].sum())\n",
    "            before_total_bboxes = int(df_comparison['before_bboxes'].sum())\n",
    "            \n",
    "            after_total_images = int(df_comparison['after_images'].sum())\n",
    "            after_total_labels = int(df_comparison['after_labels'].sum())\n",
    "            after_total_bboxes = int(df_comparison['after_bboxes'].sum())\n",
    "            \n",
    "            img_reduction = before_total_images - after_total_images\n",
    "            lbl_reduction = before_total_labels - after_total_labels\n",
    "            box_reduction = before_total_bboxes - after_total_bboxes\n",
    "            \n",
    "            img_pct = (img_reduction / before_total_images * 100) if before_total_images > 0 else 0\n",
    "            lbl_pct = (lbl_reduction / before_total_labels * 100) if before_total_labels > 0 else 0\n",
    "            box_pct = (box_reduction / before_total_bboxes * 100) if before_total_bboxes > 0 else 0\n",
    "            \n",
    "            print(f\"\\nBEFORE CLEANING:\")\n",
    "            print(f\"  Images:         {before_total_images:,}\")\n",
    "            print(f\"  Label Files:    {before_total_labels:,}\")\n",
    "            print(f\"  Bounding Boxes: {before_total_bboxes:,}\")\n",
    "            \n",
    "            print(f\"\\nAFTER CLEANING:\")\n",
    "            print(f\"  Images:         {after_total_images:,}\")\n",
    "            print(f\"  Label Files:    {after_total_labels:,}\")\n",
    "            print(f\"  Bounding Boxes: {after_total_bboxes:,}\")\n",
    "            \n",
    "            print(f\"\\nREMOVED:\")\n",
    "            print(f\"  Images:         {img_reduction:,} ({img_pct:.1f}%)\")\n",
    "            print(f\"  Label Files:    {lbl_reduction:,} ({lbl_pct:.1f}%)\")\n",
    "            print(f\"  Bounding Boxes: {box_reduction:,} ({box_pct:.1f}%)\")\n",
    "            \n",
    "            print(f\"\\nRETAINED:\")\n",
    "            print(f\"  Images:         {after_total_images:,} ({100-img_pct:.1f}%)\")\n",
    "            print(f\"  Label Files:    {after_total_labels:,} ({100-lbl_pct:.1f}%)\")\n",
    "            print(f\"  Bounding Boxes: {after_total_bboxes:,} ({100-box_pct:.1f}%)\")\n",
    "            \n",
    "            # Save detailed comparison to CSV\n",
    "            comparison_csv = export_root / 'dataset_size_comparison_detailed.csv'\n",
    "            df_comparison.to_csv(comparison_csv, index=False)\n",
    "            print(f\"\\n✅ Saved detailed comparison to: {comparison_csv}\")\n",
    "            \n",
    "            # Also save summary CSV\n",
    "            summary_csv = export_root / 'dataset_size_comparison_summary.csv'\n",
    "            summary_df = pd.DataFrame({\n",
    "                'Metric': ['Images', 'Labels', 'Bounding Boxes'],\n",
    "                'Before': [before_total_images, before_total_labels, before_total_bboxes],\n",
    "                'After': [after_total_images, after_total_labels, after_total_bboxes],\n",
    "                'Removed': [img_reduction, lbl_reduction, box_reduction],\n",
    "                'Removed_%': [f\"{img_pct:.1f}%\", f\"{lbl_pct:.1f}%\", f\"{box_pct:.1f}%\"],\n",
    "                'Retained_%': [f\"{100-img_pct:.1f}%\", f\"{100-lbl_pct:.1f}%\", f\"{100-box_pct:.1f}%\"]\n",
    "            })\n",
    "            summary_df.to_csv(summary_csv, index=False)\n",
    "            print(f\"✅ Saved summary to: {summary_csv}\")\n",
    "        else:\n",
    "            print(\"⚠️  No exported datasets found.\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "blueoasis",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
